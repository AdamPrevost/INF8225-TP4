{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TP4.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPQWKAyudjHBY2pbpxwN7Zy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ANRj2XaPXKup"},"source":["import copy\n","import time\n","import torch\n","import tensorflow as tf\n","import torch.nn as nn\n","import datetime\n","import time\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","from matplotlib import ticker\n","\n","EPSILON = np.nextafter(0, 1)\n","IMAGE_SIDE_LEN = 28\n","NB_PIXEL = 784\n","NB_PIXEL_VALUE = 256\n","NB_CATEGORY = 10\n","\n","################################## Utilities ###################################\n","# some utilities dont make sense anymore and should be removed\n","\n","def get_sets(batch_size, train_size, test_size):\n","    (xtrain, ytrain), (xtest, ytest) = #db_file\n","    xtrain = xtrain[0:train_size]\n","    ytrain = ytrain[0:train_size]\n","    xtest = xtest[0:test_size]\n","    ytest = ytest[0:test_size]\n","\n","    xtrain_tensor = torch.tensor(xtrain, dtype=torch.float, device='cuda:0')[:, :, None]\n","    ytrain_tensor = torch.tensor(ytrain, dtype=torch.long, device='cuda:0')\n","    nb_batch = math.floor(xtrain_tensor.shape[0] / batch_size)\n","    xtrain_batches = np.empty(nb_batch, dtype=object)\n","    ytrain_batches = np.empty(nb_batch, dtype=object)\n","    for i in range(0, nb_batch):\n","        xtrain_batches[i] = xtrain_tensor[i:i+batch_size]\n","        ytrain_batches[i] = ytrain_tensor[i:i+batch_size]\n","\n","    xtest_tensor = torch.tensor(xtest, dtype=torch.float, device='cuda:0')[:, :, None]\n","    ytest_tensor = torch.tensor(ytest, dtype=torch.long, device='cuda:0')\n","    nb_batch = math.floor(xtest_tensor.shape[0] / batch_size)\n","    xtest_batches = np.empty(nb_batch, dtype=object)\n","    ytest_batches = np.empty(nb_batch, dtype=object)\n","    for i in range(0, nb_batch):\n","        xtest_batches[i] = xtest_tensor[i:i + batch_size]\n","        ytest_batches[i] = ytest_tensor[i:i + batch_size]\n","\n","    xvalid_batches, xtest_batches = np.array_split(xtest_batches, 2)\n","    yvalid_batches, ytest_batches = np.array_split(ytest_batches, 2)\n","    \n","    return (xtrain_batches, ytrain_batches), (xvalid_batches, yvalid_batches), (xtest_batches, ytest_batches)\n","\n","\n","def time_since(since):\n","    now = time.time()\n","    s = now - since\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def do_confusion(model, number_batches, category_batches):\n","    confusion = torch.zeros(NB_CATEGORY, NB_CATEGORY)\n","    # Go through a bunch of examples and record which are correctly guessed\n","    with torch.no_grad():\n","        for i in range(0, number_batches.shape[0]):\n","            batch_ouput = model.forward(number_batches[i], category_batches[i])\n","            for j in range(0, batch_ouput.shape[0]):\n","                guess_index = batch_ouput[j].topk(1)[1].item()\n","                confusion[category_batches[i][j].item()][guess_index] += 1\n","    # Normalize by dividing every row by its sum\n","    for i in range(NB_CATEGORY):\n","        confusion[i] = confusion[i] / confusion[i].sum()\n","    plot_confusion(confusion)\n","\n","\n","def plot_confusion(confusion):\n","    # Set up plot\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    cax = ax.matshow(confusion.numpy())\n","    fig.colorbar(cax)\n","    # Set up axes\n","    ax.set_xticklabels([''] + list(range(NB_CATEGORY)), rotation=90)\n","    ax.set_yticklabels([''] + list(range(NB_CATEGORY)))\n","    # Force label at every tick\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"Actual\")\n","    plt.title(\"Confusion matrix\")\n","    plt.savefig(\"confusion.png\")\n","    plt.show()\n","\n","\n","def plot_failed(model, number_batches, category_batches):\n","    failed, guessed, actual = find_failed(model, number_batches, category_batches)\n","    fig = plt.figure(figsize=(11, 4))\n","    columns = 5\n","    rows = 2\n","    for i in range(columns * rows):\n","        fig.add_subplot(rows, columns, i + 1)\n","        plt.imshow(failed[i].cpu().reshape(IMAGE_SIDE_LEN, IMAGE_SIDE_LEN))\n","        plt.title(\"P: \" + str(guessed[i]) + \"  A: \" + str(actual[i]))\n","        plt.axis('off')\n","    plt.savefig(\"failed.png\")\n","    plt.show()\n","\n","\n","def find_failed(model, number_batches, category_batches):\n","    failed = []\n","    guessed = []\n","    actual = []\n","    with torch.no_grad():\n","        for i in range(0, number_batches.shape[0]):\n","            batch_ouput = model.forward(number_batches[i], category_batches[i])\n","            for j in range(0, batch_ouput.shape[0]):\n","                guess_index = batch_ouput[j].topk(1)[1].item()\n","                if category_batches[i][j].item() != guess_index:\n","                    failed.append(number_batches[i][j])\n","                    guessed.append(guess_index)\n","                    actual.append(category_batches[i][j].item())\n","                    if len(failed) == 10:\n","                        return failed, guessed, actual\n","    return failed, guessed, actual\n","\n","\n","def get_inference_time(model, number_batches, category_batches):\n","    with torch.no_grad():\n","        n1 = datetime.datetime.now()\n","        for i in range(0, number_batches.shape[0]):\n","            _ = model.forward(number_batches[i], category_batches[i])\n","    n2 = datetime.datetime.now()\n","    start = n1.second * 1000000 + n1.microsecond\n","    now = n2.second * 1000000 + n2.microsecond\n","    us = (now - start) / (number_batches.shape[0] * number_batches[0].size()[0])\n","    return str(round(us)) + \" microseconds (Î¼s)\"\n","\n","\n","def plot_accuracy(train, test, valid):\n","    plt.figure()\n","    plt.plot(train, label=\"Train\")\n","    plt.plot(test, label=\"Test\")\n","    plt.plot(valid, label=\"Validation\")\n","    plt.legend(loc='best')\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.title(\"Accuracy\")\n","    plt.savefig(\"accuracy.png\")\n","    plt.show()\n","\n","\n","def plot_loss(train, test, valid):\n","    plt.figure()\n","    plt.plot(train, label=\"Train\")\n","    plt.plot(test, label=\"Test\")\n","    plt.plot(valid, label=\"Validation\")\n","    plt.legend(loc='best')\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Loss\")\n","    plt.savefig(\"loss.png\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rG_MafD_FgNG"},"source":["################################ Neural Network ################################\n","\n","\n","class NN(nn.Module):\n","\n","    def __init__(self, model, hidden_size, output_size, lr):\n","        super(NN, self).__init__()\n","        self.base_model = model\n","        self.h2o = nn.Linear(hidden_size, output_size).cuda()\n","        self.softmax = nn.LogSoftmax(dim=1).cuda()\n","        self.loss = nn.NLLLoss().cuda()\n","        self.optimizer = torch.optim.Adam(self.parameters())\n","\n","\n","    def forward(self, input, _): # 3rd param is to fit with Transformer.forward\n","        output, _ = self.base_model.forward(input)\n","        output = self.h2o(output[:, -1, :])\n","        output = self.softmax(output)\n","        return output\n","\n","\n","class Transformer(nn.Module):\n","\n","    def __init__(self, model, batch_size):\n","        super(Transformer, self).__init__()\n","        self.base_model = model\n","        self.loss = nn.NLLLoss().cuda()\n","        self.optimizer = torch.optim.Adam(self.parameters())\n","        self.class_tokens = torch.tensor(np.zeros((batch_size, NB_PIXEL, NB_CATEGORY)), device='cuda:0')\n","        self.input_token = torch.tensor(np.zeros((batch_size, 1)), device='cuda:0')\n","\n","    def forward(self, input, target):\n","        input = torch.cat((input, self.class_tokens), 2).transpose(0, 1).float()\n","        target = torch.cat((self.input_token, nn.functional.one_hot(target, NB_CATEGORY)), 1).float()[None, :]\n","        output = self.base_model.forward(input, target)\n","        output = torch.squeeze(output, 0)[:, 1:]\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f77dHUhO0Nvu"},"source":["################################ Training ######################################\n","\n","def train_batch(model, number_batch, category_batch):\n","    model.zero_grad()\n","    model.optimizer.zero_grad()\n","    output = model.forward(number_batch, category_batch)\n","    loss = model.loss(output, category_batch)\n","    loss.backward()\n","    model.optimizer.step()\n","    return output, loss.item()\n","\n","\n","def get_accuracy_loss(model, number_batches, category_batches):\n","    nb_correct = 0\n","    with torch.no_grad():\n","        for i in range(0, number_batches.shape[0]):\n","            batch_ouput = model.forward(number_batches[i], category_batches[i])\n","            loss = model.loss(batch_ouput, category_batches[i])\n","            for j in range(0, batch_ouput.shape[0]):\n","                if batch_ouput[j].topk(1)[1].item() == category_batches[i][j]:\n","                    nb_correct += 1\n","    return nb_correct / (number_batches.shape[0] * number_batches[0].size()[0]), loss\n","\n","\n","def log_accuracy_loss(model, number_batches, category_batches, accuracy_array, loss_array):\n","    accuracy, loss = get_accuracy_loss(model, number_batches, category_batches)\n","    accuracy_array.append(accuracy)\n","    loss_array.append(loss)\n","    return accuracy, loss\n","\n","\n","def log_sets(model, epoch, nb_epochs):\n","    train_accuracy, train_loss = log_accuracy_loss(model, xtrain, ytrain, train_accuracies, train_losses)\n","    test_accuracy, _ = log_accuracy_loss(model, xtest, ytest, test_accuracies, test_losses)\n","    validation_accuracy, _ = log_accuracy_loss(model, xvalid, yvalid, validation_accuracies, validation_losses)\n","    print('%d [%d%%][%s] Loss: %.4f Train:%.2f%% Test:%.2f%%' % (epoch, int(epoch / nb_epochs * 100), time_since(start),\n","                                                      train_loss, train_accuracy * 100, test_accuracy * 100))\n","    return validation_accuracy\n","\n","\n","def train(model):\n","    best_model = model\n","    best_validation_accuracy = log_sets(model, 0, nb_epochs)\n","    nb_batch = xtrain.shape[0]\n","    for epoch in range(0, nb_epochs):\n","        for i in range(0, nb_batch):\n","            _, _ = train_batch(model, xtrain[i], ytrain[i])\n","        validation_accuracy = log_sets(model, epoch+1, nb_epochs)\n","        if validation_accuracy > best_validation_accuracy:\n","            best_validation_accuracy = validation_accuracy\n","            best_model = copy.deepcopy(model)\n","    return best_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"irI2uUHSFKJj"},"source":["################################## Setup #######################################\n","\n","cuda0 = torch.device('cuda:0')\n","\n","# Parameters\n","batch_size = 100\n","nb_epochs = 30\n","train_size = 60000\n","test_size = 10000\n","hidden_size = 6\n","num_layers = 1\n","lr = 0.001\n","\n","# !! uncomment the model you want to run, training is generic and models are interchangeable !!\n","\n","# LSTM, GRU\n","#base_model = nn.LSTM(1, hidden_size, num_layers, batch_first=True).cuda()\n","#base_model = nn.GRU(1, hidden_size, num_layers, batch_first=True).cuda()\n","#model = NN(base_model, hidden_size, NB_CATEGORY, lr)\n","#(xtrain, ytrain), (xvalid, yvalid), (xtest, ytest) = get_sets(batch_size, train_size, test_size)\n","\n","# Transformer\n","tranformer_model = nn.Transformer(d_model=1+NB_CATEGORY, nhead=1, dim_feedforward=hidden_size).cuda()\n","model = Transformer(tranformer_model, batch_size)\n","(xtrain, ytrain), (xvalid, yvalid), (xtest, ytest) = get_sets(batch_size, train_size, test_size)\n","\n","# Logs\n","train_accuracies = []\n","train_losses = []\n","validation_accuracies = []\n","validation_losses = []\n","test_accuracies = []\n","test_losses = []\n","\n","# Train\n","start = time.time()\n","best_model = train(model)\n","final_time = time_since(start)\n","\n","# Results\n","print('Final train accuracy: %.2f%%' % (get_accuracy_loss(best_model, xtrain, ytrain)[0]*100))\n","print('Final test accuracy: %.2f%%' % (get_accuracy_loss(best_model, xtest, ytest)[0]*100))\n","print('Overall training time: %s' % (final_time))\n","print('Average inference time: %s' % (get_inference_time(best_model, xtest, ytest)))\n","print('Number of parameters: %d' % (sum(p.numel() for p in best_model.parameters())))\n","do_confusion(best_model, xtest, ytest)\n","plot_loss(train_losses, test_losses, validation_losses)\n","plot_accuracy(train_accuracies, test_accuracies, validation_accuracies)\n","plot_failed(best_model, xtest, ytest)"],"execution_count":null,"outputs":[]}]}